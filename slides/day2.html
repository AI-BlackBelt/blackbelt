<!DOCTYPE html>
<html>
  <head>
    <title>AI Black Belt - Yellow</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="./assets/katex.min.css">
    <link rel="stylesheet" href="./assets/style.css">
    <link rel="stylesheet" href="./assets/grid.css">
  </head>
  <body>
    <textarea id="source">
class: middle, center, title-slide

# Yellow belt

Day 2: Learn to identify and solve supervised learning problems

---

class: middle

## Outline

.inactive[[Day 1] Introduction to machine learning with Python]

[Day 2] Learn to identify and solve supervised learning problems
- Learn to recognize classification and regression problems in the wild.
- Practice classification algorithms with Scikit-Learn.
- Learn to evaluate properly the performance of your models.

.inactive[[Day 3] Practice regression algorithms for sentiment analysis]

.inactive[[Day 4] Learn how to let the machine tune itself]

---

class: middle

# Supervised learning

---

class: middle

.center.width-100[![](figures/day2/map3.jpg)]

.footnote[Credits: vas3k, [Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/), 2018.]


---

# Representing data

<br>
.center.width-100[![](figures/day2/matrix-representation.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-1-of-4/), 2019.]

---

# Supervised learning

.center.width-70[![](figures/day2/classification.jpg)]

.footnote[Credits: vas3k, [Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/), 2018.]

---

class: middle

.center.width-70[![](figures/day2/regression.jpg)]

.footnote[Credits: vas3k, [Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/), 2018.]

---

class: middle

Formally, given inputs $\mathbf{X}$ and outputs $\mathbf{y}$, we want to find a function $f$ such that $\mathbf{y}=f(\mathbf{X})$.
- in classification, the set of possible output values is finite and *symbolic*.
- in regression, the set of possible output values is infinite and **numerical**.

---

# In the wild

.grid[
.kol-1-2[
## Classification

- Visual recognition
- Spam filtering
- Sentiment analysis
- Medical diagnosis
- Weather forecast
- Customer segmentation
- Recommendations


]
.kol-1-2[
## Regression

- Weather forecast
- Stock price forecast
- Demand and sales volume analysis
- Estimating the price of an Uber ride
- Market value prediction
- Playing games

]
]

.exercice[Can you think of more examples?]

---

class: middle

.exercice[Is this a classification or a regression problem? What are the inputs and outputs?]

---

class: middle, center, black-slide

.width-45[![](figures/day2/titanic1.jpg)]

.width-45[![](figures/day2/titanic2.png)]

Would you he survive the sinking of the Titanic?


---

class: middle, center, black-slide

.width-55[![](figures/day2/cucumber1.png)]

.width-55[![](figures/day2/cucumber2.png)]

Sorting cucumbers?

---

class: middle, center, black-slide

.width-80[![](figures/day2/jam.jpg)]

How long will it take to arrive at work if you leave home at 6:00 AM?

---

class: middle, center, black-slide

.width-80[![](figures/day2/sick.jpg)]

Are you sick?

---

class: middle, center, black-slide

.width-80[![](figures/day2/castle.jpg)]

What's the market value of my house?

---

class: middle

.exercice[Identify supervised learning problems for each of the following situations. What are the inputs and outputs?]

---

class: middle, black-slide

.center.width-80[![](figures/day2/fb1.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-1-of-4/), 2019.]

---

class: middle, black-slide

.center.width-80[![](figures/day2/facebook_gael.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-1-of-4/), 2019.]


---

class: middle, center, black-slide

.width-80[![](figures/day2/amazon.png)]

---

class: middle, center, black-slide

.width-80[![](figures/day2/tesla.jpeg)]

---

class: middle, center, black-slide

.width-40[![](figures/day2/pickit.jpg)]

---

class: middle, center, black-slide

.width-80[![](figures/day2/eeg.jpg)]

---

class: middle

# Classification

---

# Recap

.grid[

.kol-1-2[
<br><br><br><br>

## Estimator API

```python
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
clf.score(X_test, y_test)
```
]
.kol-1-2[
.center.width-100[![](figures/day2/supervised-ml-flow.svg)]
]

]

---

class: middle

Jump to `day2-01-scikit-learn.ipynb`.

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AI-BlackBelt/yellow/master)

---

# Choosing the right estimator

<br><br>
.center.width-100[![](figures/day2/ml_map.png)]

---

class: middle

## Linear model

 Model the decision boundary as an hyperplane.

.center.width-100[![](figures/day2/linear-model.jpg)]

.footnote[Credits: vas3k, [Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/), 2018.]

---

class: middle

.center.width-100[![](figures/day2/illus-lr.png)]

```python
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression()
clf.fit(X, y)
```

---

class: middle

## K-Nearest neighbors

Predict the average output value among the $K$ closest neighbors to the input $\mathbf{x}$. Closeness is typically defined using the Euclidean distance.

.center.width-50[![](figures/day2/knn.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-1-of-4/), 2019.]

---

class: middle

.center.width-100[![](figures/day2/illus-knn.png)]

```python
from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors=5)
clf.fit(X, y)
```

---

class: middle

## Naive Bayes

Idea: apply the Bayes rule to make predictions for $y$ given $\mathbf{x}$.

.center.width-100[![](figures/day2/naive-bayes.jpg)]

.footnote[Credits: vas3k, [Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/), 2018.]

---

class: middle

.center.width-100[![](figures/day2/illus-nb.png)]

```python
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X, y)
```

---

class: middle

## Decision trees

Idea: build a partition of the input space using cuts orthogonal to feature axes.

.center.width-100[![](figures/day2/tree.jpg)]

.footnote[Credits: vas3k, [Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/), 2018.]

---

class: middle

.center.width-100[![](figures/day2/illus-tree.png)]

```python
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()
clf.fit(X, y)
```

---

# Preprocessing

Not only the performance on your system depends on the algorithm you pick, but also on how you **prepare** the data.

- How should you treat missing values?
- Does the scale of the features matter?
- How do you encode categorical/symbolic features into numerical values?

<br>
.center.width-50[![](figures/day2/gigo.jpg)]
.center.italic[Garbage in, garbage out.]

---

class: middle

## Transformer API

```python
tf = StandardScaler()
tf.fit(X_train)
X_train_scaled = tf.transform(X_train)
X_test_scaled = tf.transform(X_test)
```

<br>
.center.width-50[![](figures/day2/transformer-flow.svg)]


---

class: middle

Jump to `day2-02-census.ipynb`.

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AI-BlackBelt/yellow/master)

---

class: middle

# Model evaluation

---

# Metrics

Estimators in Scikit-Learn come with a built-in default evaluation score:
- **accuracy** for classification
- *R2 score* for regression

Example:

```python
clf = KNeighborsClassifier(n_neighbors=5)
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))
>>> 0.84
```

---

class: middle

## Accuracy

The accuracy is the proportion of correct predictions.

Example:

```python
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, clf.predict(X_test)))
>>> 0.84
```

.exercice[Is this an appropriate measure when classes are imbalanced?]

---

class: middle

## Precision, recall and F-measure


.grid[
.kol-1-2.smaller-x[

<br>
$$
\begin{aligned}
\text{Precision} &= \frac{TP}{TP + FP} \\\\
\text{Recall} &= \frac{TP}{TP + FN} \\\\
F &= \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{aligned}
$$

]
.kol-1-2[
.center.width-100[![](figures/day2/pr.png)]
]
]


```python
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import fbeta_score
print("Precision =", precision_score(y_test, clf.predict(X_test)))
print("Recall =", recall_score(y_test, clf.predict(X_test)))
print("F =", fbeta_score(y_test, clf.predict(X_test), beta=1))
>>> Precision = 0.8118811881188119
>>> Recall = 0.8631578947368421
>>> F = 0.8367346938775511
```



---

class: middle

## ROC AUC

Area under the curve of the false positive rate (FPR) against the true positive rate (TPR) as the decision threshold of the classifier is varied.

.grid[
.kol-1-2[
.center.width-100[![](figures/day2/roc-auc.png)]
]
.kol-1-2[
```python
from sklearn.metrics import roc_auc_score
print("ROC AUC =", roc_auc_score(y_test, clf.predict_proba(X_test, y_test)[:, 1])
>>> ROC AUC = 0.9297744360902256
```
]
]

---

# Training and test data

.center.width-70[![](figures/day2/train-test-split.png)]

Why splitting?
- The error estimated on training data is typically optimistic. It **does not** accurately represent the performance the model would have on new data.
- The actual performance is estimated on (independent) test data.

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-1-of-4/), 2019.]

---

class: middle

Jump to
- `day2-03-evaluation.ipynb`
- `day2-04-challenge.ipynb`

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AI-BlackBelt/yellow/master)


    </textarea>
    <script src="./assets/remark-latest.min.js"></script>
    <script src="./assets/auto-render.min.js"></script>
    <script src="./assets/katex.min.js"></script>

    <script type="text/javascript">
        var options = {highlightStyle: "tomorrow"};
        var renderMath = function() {
            renderMathInElement(document.body, {delimiters: [ // mind the order of delimiters(!?)
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\[", right: "\\]", display: true},
                {left: "\\(", right: "\\)", display: false},
            ]});
        }
      var slideshow = remark.create(options, renderMath);
    </script>
  </body>
</html>
