<!DOCTYPE html>
<html>
  <head>
    <title>AI Black Belt - Yellow</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="./assets/katex.min.css">
    <link rel="stylesheet" href="./assets/style.css">
    <link rel="stylesheet" href="./assets/grid.css">
  </head>
  <body>
    <textarea id="source">
class: middle, center, title-slide

.center[.width-60[![](./assets/yellow.png)]]

# AI Black Belt - Yellow

Day 4/4: Learn how to let the machine tune itself

<br><br><br>



---

class: middle

## Outline

.inactive[[Day 1] Introduction to machine learning with Python]

.inactive[[Day 2] Learn to identify and solve supervised learning problems]

.inactive[[Day 3] Practice regression algorithms for sentiment analysis]

[Day 4] Learn how to let the machine tune itself
- Learn how to automatically tune and evaluate your models.
- Get familiar with intermediate ML algorithms.
- Learn how to structure your ML pipeline with Scikit-Learn.

---

class: middle

# Model selection and evaluation

---

# Effect of `n_neighbors`

<br>
.center.width-50[![](figures/day4/knn_boundary_k1.png)]
.center[`n_neighbors=1`]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

count: false

# Effect of `n_neighbors`

<br>
.center.width-50[![](figures/day4/knn_boundary_k3.png)]
.center[`n_neighbors=3`]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

.center.width-60[![](figures/day4/knn_boundary_varying_k.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

# Model complexity

.center.width-100[![](figures/day4/knn_model_complexity.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]


---

# Overfitting and underfitting

<br>
.center.width-100[![](figures/day4/overfitting_underfitting_cartoon_train.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

count: false

# Overfitting and underfitting

<br>
.center.width-100[![](figures/day4/overfitting_underfitting_cartoon_generalization.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

count: false

# Overfitting and underfitting

<br>
.center.width-100[![](figures/day4/overfitting_underfitting_cartoon_full.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

# Model selection and evaluation

.center.width-80[![](figures/day4/train-test-split.png)]

.exercice[How to do both hyper-parameter tuning and model evaluation?]

---

class: middle

## Threefold split

.center.width-100[![](figures/day4/threefold_split.png)]

- pro: fast, simple
- con: high variance, bad use of data

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

## Cross-validation

.center.width-100[![](figures/day4/kfold_cv.png)]

- pro: more stable, more data
- con: slower

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

## Cross-validation + test set

.center.width-80[![](figures/day4/grid_search_cross_validation.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

## Grid search

.center.width-80[![](figures/day4/gridsearch_workflow.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

Jump to `day4-01-model-selection.ipynb`.

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AI-BlackBelt/yellow/master)

---

class: middle

# Trees, random forests and boosting

---

# Decision trees

<br><br>
.center.width-60[![](figures/day4/tree_illustration.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

## Tree construction

.center.width-100[![](figures/day4/tree_building_iteration_1.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]


---

count: false
class: middle

## Tree construction

.center.width-100[![](figures/day4/tree_building_iteration_2.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]


---

count: false
class: middle

## Tree construction

.center.width-100[![](figures/day4/tree_building_iteration_9.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

## Important parameters

- `max_depth`
- `max_leaf_nodes`
- `min_samples_split`
- `min_impurity_decrease`

---

class: middle

.center.width-100[![](figures/day4/no_pruning.png)]

.center[No pruning]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

.center.width-100[![](figures/day4/max_depth_4.png)]

.center[`max_depth=4`]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

.center.width-60[![](figures/day4/max_leaf_nodes_8.png)]

.center[`max_leaf_nodes=8`]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

.center.width-100[![](figures/day4/min_samples_split_50.png)]

.center[`min_samples_split=50`]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]


---

class: middle

## Feature importances

.grid[
.kol-1-2[.width-100[![](figures/day4/instability_1.png)]]
.kol-1-2[.width-100[![](figures/day4/tree_importances.png)]]
]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]


---

# Random forests

<br>
.center.width-100[![](figures/day4/random_forest.png)]

.center[Build $T$ randomized trees and average their predictions.]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle, black-slide

.grid[
.kol-1-2[
<br><br><br>

## Condorcet Jury theorem

For a jury of $M$ (independent) members, each with a probability $p$ of being right, the probability $\mu$ of the majority decision to be right tends to 1 as $M\to \infty$ if $p>0.5$.

]
.kol-1-2[.center.width-90[![](figures/day4/condorcet.png)]]
]





---

class: middle

## Important parameters

- `n_estimators`: the more, the better
- `max_features`: control the amount of randomness

---

# Gradient boosting

<br><br>

$$
\begin{aligned}
f\_{1}(x) &\approx y \\\\
f\_{2}(x) &\approx y - f\_{1}(x) \\\\
f\_{3}(x) &\approx y - f\_{1}(x) - f\_{2}(x)
\end{aligned}$$

$y \approx$ .width-30[![](figures/day4/grad_boost_term_1.png)] + .width-30[![](figures/day4/grad_boost_term_2.png)] + .width-15[![](figures/day4/grad_boost_term_3.png)] + ...

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]


---

count: false

# Gradient boosting

<br><br>

$$
\begin{aligned}
f\_{1}(x) &\approx y \\\\
f\_{2}(x) &\approx y - \gamma f\_{1}(x) \\\\
f\_{3}(x) &\approx y - \gamma f\_{1}(x) - \gamma f\_{2}(x)
\end{aligned}$$

$y \approx$ $\gamma$ .width-30[![](figures/day4/grad_boost_term_1.png)] + $\gamma$ .width-30[![](figures/day4/grad_boost_term_2.png)] + $\gamma$ .width-15[![](figures/day4/grad_boost_term_3.png)] + ...

$\gamma$ = learning rate

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

.center.width-100[![](figures/day4/boosting.png)]

---

class: middle

## Important parameters

- `n_estimators` and `learning_rate`
- `max_depth`
- `max_features`

---

class: middle

Jump to `day4-02-forest-boosting.ipynb`.

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AI-BlackBelt/yellow/master)

---

class: middle

# Yellow belt final test!

---

class: middle

Jump to `day4-03-yellow-belt-test.ipynb`.

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AI-BlackBelt/yellow/master)

---

class: middle

# Wrap-up

---

class: middle

## Table of contents

[Day 1] Introduction to machine learning with Python
- Overview of artificial intelligence and machine learning
- Get familiar with the data Python ecosystem.
- Train your first ML model.

[Day 2] Learn to identify and solve supervised learning problems
- Learn to recognize classification and regression problems in the wild.
- Practice classification algorithms with Scikit-Learn.

---

class: middle

[Day 3] Practice regression algorithms for sentiment analysis
- Learn to evaluate properly the performance of your models.
- Practice regression algorithms with Scikit-Learn.
- Implement a full ML pipeline: from raw to text to sentiment analysis.

[Day 4] Learn how to let the machine tune itself
- Learn how to automatically tune and evaluate your models.
- Get familiar with intermediate ML algorithms.
- Learn how to structure your ML pipeline with Scikit-Learn.

---

class: middle

.center.width-70[![](figures/day1/map2-yellow.jpg)]

.footnote[Credits: vas3k, [Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/), 2018.]

---

class: middle

.center.width-100[![](figures/day1/ai-cycle-yellow.png)]

---

class: middle, black-slide

.center.width-80[![](figures/day4/expectations.jpg)]

---

count: false
class: black-slide, middle, center

The end.

    </textarea>
    <script src="./assets/remark-latest.min.js"></script>
    <script src="./assets/auto-render.min.js"></script>
    <script src="./assets/katex.min.js"></script>

    <script type="text/javascript">
        var options = {highlightStyle: "tomorrow"};
        var renderMath = function() {
            renderMathInElement(document.body, {delimiters: [ // mind the order of delimiters(!?)
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\[", right: "\\]", display: true},
                {left: "\\(", right: "\\)", display: false},
            ]});
        }
      var slideshow = remark.create(options, renderMath);
    </script>
  </body>
</html>
