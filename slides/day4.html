<!DOCTYPE html>
<html>
  <head>
    <title>AI Black Belt - Yellow</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="./assets/katex.min.css">
    <link rel="stylesheet" href="./assets/style.css">
    <link rel="stylesheet" href="./assets/grid.css">
  </head>
  <body>
    <textarea id="source">
class: middle, center, title-slide

.center[.width-60[![](./assets/yellow.png)]]

## AI Black Belt - Yellow
Day 4/4: Learn how to let the machine tune itself



---

class: middle

## Outline

.inactive[[Day 1] Introduction to machine learning with Python]

.inactive[[Day 2] Learn to identify and solve supervised learning problems]

.inactive[[Day 3] Practice regression algorithms for sentiment analysis]

[Day 4] Learn how to let the machine tune itself
- Learn how to automatically tune and evaluate your models.
- Get familiar with intermediate ML algorithms.
- Learn how to structure your ML pipeline with Scikit-Learn.

---

class: middle

# Model selection and evaluation

---

# Effect of `n_neighbors`

<br>
.center.width-50[![](figures/day4/knn_boundary_k1.png)]
.center[`n_neighbors=1`]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

count: false

# Effect of `n_neighbors`

<br>
.center.width-50[![](figures/day4/knn_boundary_k3.png)]
.center[`n_neighbors=3`]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

.center.width-60[![](figures/day4/knn_boundary_varying_k.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

# Model complexity

.center.width-100[![](figures/day4/knn_model_complexity.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]


---

# Overfitting and underfitting

<br>
.center.width-100[![](figures/day4/overfitting_underfitting_cartoon_train.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

count: false

# Overfitting and underfitting

<br>
.center.width-100[![](figures/day4/overfitting_underfitting_cartoon_generalization.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

count: false

# Overfitting and underfitting

<br>
.center.width-100[![](figures/day4/overfitting_underfitting_cartoon_full.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

# Model selection and evaluation

.center.width-80[![](figures/day4/train-test-split.png)]

.exercice[How to do both hyper-parameter tuning and model evaluation?]

---

class: middle

## Threefold split

.center.width-100[![](figures/day4/threefold_split.png)]

- pro: fast, simple
- con: high variance, bad use of data

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

```python
X_trainval, X_test, y_trainval, y_test = train_test_split(X, y)
X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval)
val_scores = []
neighbors = np.arange(1, 15, 2)

for i in neighbors:
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    val_scores.append(knn.score(X_val, y_val))

print("best validation score: {:.3f}".format(np.max(val_scores)))
best_n_neighbors = neighbors[np.argmax(val_scores)]
print("best n_neighbors:", best_n_neighbors)

knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)
knn.fit(X_trainval, y_trainval)
print("test-set score: {:.3f}".format(knn.score(X_test, y_test)))

>>> best validation score: 0.991
>>> best n_neighbors: 11
>>> test-set score: 0.951
```

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

## Cross-validation

.center.width-100[![](figures/day4/kfold_cv.png)]

- pro: more stable, more data
- con: slower

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

.center.width-100[![](figures/day4/grid_search_cross_validation.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

```python
from sklearn.model_selection import cross_val_score

X_train, X_test, y_train, y_test = train_test_split(X, y)
cross_val_scores = []

for i in neighbors:
    knn = KNeighborsClassifier(n_neighbors=i)
    scores = cross_val_score(knn, X_train, y_train, cv=10)
    cross_val_scores.append(np.mean(scores))

print("best cross-validation score: {:.3f}".format(np.max(cross_val_scores)))
best_n_neighbors = neighbors[np.argmax(cross_val_scores)]
print("best n_neighbors:", best_n_neighbors)

knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)
knn.fit(X_train, y_train)
print("test-set score: {:.3f}".format(knn.score(X_test, y_test)))

>>> best cross-validation score: 0.967
>>> best n_neighbors: 9
>>> test-set score: 0.965
```

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

## Grid search

.center.width-80[![](figures/day4/gridsearch_workflow.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]


---

class: middle

```python
from sklearn.model_selection import GridSearchCV

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)

param_grid = {'n_neighbors':  np.arange(1, 15, 2)}
grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid,
                    cv=10, return_train_score=True)
grid.fit(X_train, y_train)

print("best mean cross-validation score: {:.3f}".format(grid.best_score_))
print("best parameters: {}".format(grid.best_params_))
print("test-set score: {:.3f}".format(grid.score(X_test, y_test)))

>>> best mean cross-validation score: 0.967
>>> best parameters: {'n_neighbors': 9}
>>> test-set score: 0.993
```

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]


---

class: middle

Jump to `day4-01-model-selection.ipynb`.

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AI-BlackBelt/yellow/master)

---

class: middle

# Trees, random forests and boosting

---

# Decision trees

<br><br>
.center.width-60[![](figures/day4/tree_illustration.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

## Construction

.center.width-60[![](figures/day4/tree_building_iteration_1.png)]
.center.width-60[![](figures/day4/tree_building_iteration_2.png)]
.center.width-60[![](figures/day4/tree_building_iteration_9.png)]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

## Important parameters

- `max_depth`
- `max_leaf_nodes`
- `min_samples_split`
- `min_impurity_decrease`

---

class: middle

.center.width-100[![](figures/day4/no_pruning.png)]

.center[No pruning]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

.center.width-100[![](figures/day4/max_depth_4.png)]

.center[`max_depth=4`]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

.center.width-60[![](figures/day4/max_leaf_nodes_8.png)]

.center[`max_leaf_nodes=8`]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

.center.width-100[![](figures/day4/min_samples_split_50.png)]

.center[`min_samples_split=50`]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]


---

class: middle

## Feature importances

.grid[
.kol-1-2[.width-100[![](figures/day4/instability_1.png)]]
.kol-1-2[.width-100[![](figures/day4/tree_importances.png)]]
]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]


---

# Random forests

<br>
.center.width-100[![](figures/day4/random_forest.png)]

.center[Build $T$ randomized trees and average their predictions.]

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

## Important parameters

- `n_estimators`: the more, the better
- `max_features`: control the amount of randomness

---

# Gradient boosting

<br><br><br>

$$
\begin{aligned}
f\_{1}(x) &\approx y \\\\
f\_{2}(x) &\approx y - f\_{1}(x) \\\\
f\_{3}(x) &\approx y - f\_{1}(x) - f\_{2}(x)
\end{aligned}$$

$y \approx$ .width-25[![](figures/day4/grad_boost_term_1.png)] + .width-25[![](figures/day4/grad_boost_term_2.png)] + .width-20[![](figures/day4/grad_boost_term_3.png)] + ...

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]


---

count: false

# Gradient boosting

<br><br><br>

$$
\begin{aligned}
f\_{1}(x) &\approx y \\\\
f\_{2}(x) &\approx y - \gamma f\_{1}(x) \\\\
f\_{3}(x) &\approx y - \gamma f\_{1}(x) - \gamma f\_{2}(x)
\end{aligned}$$

$y \approx$ $\gamma$ .width-25[![](figures/day4/grad_boost_term_1.png)] + $\gamma$ .width-25[![](figures/day4/grad_boost_term_2.png)] + $\gamma$ .width-20[![](figures/day4/grad_boost_term_3.png)] + ...

$\gamma$ = learning rate

.footnote[Credits: Andreas Mueller, [Introduction to Machine Learning with Scikit-Learn](https://github.com/amueller/ml-workshop-2-of-4/), 2019.]

---

class: middle

.center.width-100[![](figures/day4/boosting.png)]

---

class: middle

## Advantages

- Slower to train than RF (if using "old" `GradientBoostingRegressor`), but much faster to predict.
- Very fast using XGBoost, LightGBM, pygbm, new scikit-learn implementation `HistGradientBoosting`.
- Small model size.
- Typically the **best off-the-shelf model** for tabular data.


---

class: middle

## Important parameters

- `n_estimators` and `learning_rate`
- `max_depth`
- `max_features`

---

class: middle

Jump to `day4-02-forest-boosting.ipynb`.

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AI-BlackBelt/yellow/master)

---

class: middle

# Yellow belt final test!

---

class: middle

Jump to `day4-03-yellow-belt-test.ipynb`.

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AI-BlackBelt/yellow/master)

---

class: middle

# Summary

---

xxx

    </textarea>
    <script src="./assets/remark-latest.min.js"></script>
    <script src="./assets/auto-render.min.js"></script>
    <script src="./assets/katex.min.js"></script>

    <script type="text/javascript">
        var options = {highlightStyle: "tomorrow"};
        var renderMath = function() {
            renderMathInElement(document.body, {delimiters: [ // mind the order of delimiters(!?)
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\[", right: "\\]", display: true},
                {left: "\\(", right: "\\)", display: false},
            ]});
        }
      var slideshow = remark.create(options, renderMath);
    </script>
  </body>
</html>
