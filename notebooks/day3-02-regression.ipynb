{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression \n",
    "\n",
    "AI Black Belt - Yellow (June 2019).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:24:17.859108Z",
     "start_time": "2019-05-31T08:24:17.436201Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression we are trying to predict a continuous output variable -- in contrast to the nominal variables we were predicting in the previous classification examples. \n",
    "\n",
    "Let's start with a simple toy example with one feature dimension (explanatory variable) and one target variable. We will create a dataset out of a sine curve with some noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:25:47.429023Z",
     "start_time": "2019-05-31T08:25:47.416400Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x\":np.linspace(-3, 3, 100)})\n",
    "print(df['x'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:26:43.153432Z",
     "start_time": "2019-05-31T08:26:43.081894Z"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "df[\"y\"] = df[\"x\"] + np.sin(4 * df[\"x\"]) + rng.uniform(size=df.shape[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:26:57.759946Z",
     "start_time": "2019-05-31T08:26:57.494908Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(df[\"x\"], df[\"y\"], 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "The first model that we will introduce is the so-called simple linear regression. Here, we want to fit a line to the data, so that the error over the data points is minimized. \n",
    "\n",
    "The interface for LinearRegression is exactly the same as for the classifiers before, only that ``y`` now contains float values, instead of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:28:09.997981Z",
     "start_time": "2019-05-31T08:28:09.987745Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df[[\"x\"]]\n",
    "y = df[[\"y\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we start by splitting our dataset into a training (75%) and a test set (25%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:28:11.531514Z",
     "start_time": "2019-05-31T08:28:10.829647Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the learning algorithm implemented in `LinearRegression` to **fit a regression model to the training data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:28:12.441159Z",
     "start_time": "2019-05-31T08:28:12.322469Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our regression model is a linear one, the relationship between the target variable (y) and the feature variable (x) is defined as \n",
    "\n",
    "$$y = \\text{weight} \\times x + \\text{intercept .}$$\n",
    "\n",
    "In our case, for the model fit above, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Weight coefficients: ', regressor.coef_)\n",
    "print('y-axis intercept: ', regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging in the min and max values into thos equation, we can plot the regression fit to our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:28:19.408624Z",
     "start_time": "2019-05-31T08:28:19.204416Z"
    }
   },
   "outputs": [],
   "source": [
    "min_pt = X.min() * regressor.coef_[0] + regressor.intercept_\n",
    "max_pt = X.max() * regressor.coef_[0] + regressor.intercept_\n",
    "\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt])\n",
    "plt.plot(X_train, y_train, 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the estimators for classification in the previous notebooks, we use the `predict` method to predict the target variable. We expect these predicted values to fall onto the line that we plotted previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:28:26.201908Z",
     "start_time": "2019-05-31T08:28:26.194781Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_train = regressor.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:28:27.945466Z",
     "start_time": "2019-05-31T08:28:27.693519Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_train, y_train, 'o', label=\"data\")\n",
    "plt.plot(X_train, y_pred_train, 'o', label=\"prediction\")\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt], label='fit')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the plot above, the line is able to capture the general slope of the data, but not many details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:28:52.276422Z",
     "start_time": "2019-05-31T08:28:52.268057Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:28:53.025204Z",
     "start_time": "2019-05-31T08:28:52.724677Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_test, y_test, 'o', label=\"data\")\n",
    "plt.plot(X_test, y_pred_test, 'o', label=\"prediction\")\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt], label='fit')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, scikit-learn provides an easy way to evaluate the prediction quantitatively using the ``score`` method. For regression tasks, this is the R<sup>2</sup> score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:28:56.073072Z",
     "start_time": "2019-05-31T08:28:56.064500Z"
    }
   },
   "outputs": [],
   "source": [
    "regressor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular way would be the Mean Squared Error (MSE). As its name implies, the MSE is simply the average squared difference over the predicted and actual target values\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\text{predicted}_i - \\text{true}_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Add a (non-linear) feature containing  `sin(4x)` to `X` and redo the fit as a new column to X_train (and X_test). Visualize the predictions with this new richer, yet linear, model.\n",
    "      </li>\n",
    "      <li>\n",
    "      Hint: you can use `np.concatenate(A, B, axis=1)` to concatenate two matrices A and B horizontal (to combine the columns).\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:40:44.271519Z",
     "start_time": "2019-05-31T08:40:44.267531Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/day3-02-01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsRegression\n",
    "\n",
    "As for classification, we can also use a K-nearest neighbor algorithm for regression. In this setting, we can simply take the output of the nearest point, or we can average several nearest points. This method is less popular for regression than for classification, but it is still a good baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:41:38.761109Z",
     "start_time": "2019-05-31T08:41:38.747977Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df[[\"x\"]]\n",
    "y = df[\"y\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:41:41.072344Z",
     "start_time": "2019-05-31T08:41:41.061013Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "kneighbor_regression = KNeighborsRegressor(n_neighbors=1)\n",
    "kneighbor_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let us look at the behavior on training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:41:42.299157Z",
     "start_time": "2019-05-31T08:41:41.991324Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_train = kneighbor_regression.predict(X_train)\n",
    "\n",
    "plt.plot(X_train, y_train, 'o', label=\"data\", markersize=10)\n",
    "plt.plot(X_train, y_pred_train, 's', label=\"prediction\", markersize=4)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the training set, we do a perfect job: each point is its own nearest neighbor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:41:45.463355Z",
     "start_time": "2019-05-31T08:41:45.184615Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test = kneighbor_regression.predict(X_test)\n",
    "\n",
    "plt.plot(X_test, y_test, 'o', label=\"data\", markersize=8)\n",
    "plt.plot(X_test, y_pred_test, 's', label=\"prediction\", markersize=4)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test set, we also do a better job of capturing the variation, but our estimates look much messier than before.\n",
    "Let us look at the R<sup>2</sup> score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:41:46.433006Z",
     "start_time": "2019-05-31T08:41:46.423194Z"
    }
   },
   "outputs": [],
   "source": [
    "kneighbor_regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better than before! Here, the linear model was not a good fit for our problem; it was lacking in complexity and thus under-fit our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:45:56.513765Z",
     "start_time": "2019-05-31T08:45:56.505788Z"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "\n",
    "Check on <a href=http://scikit-learn.org/stable/supervised_learning.html#supervised-learning>scikit-learn documentation</a> what other regression model exist, choose one an try it to see if it improves the accuracy.\n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
