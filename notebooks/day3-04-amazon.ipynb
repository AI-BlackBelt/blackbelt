{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting review scores \n",
    "\n",
    "AI Black Belt - Yellow (June 2019).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T14:02:11.814036Z",
     "start_time": "2019-05-31T14:02:11.794965Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:59:42.278762Z",
     "start_time": "2019-05-31T13:59:42.048656Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/amazon-reviews.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Feature Extraction with Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many tasks, like in the classical spam detection, your input data is text.\n",
    "Free text with variables length is very far from the fixed length numeric representation that we need to do machine learning with scikit-learn.\n",
    "However, there is an easy and effective way to go from text data to a numeric representation using the so-called bag-of-words model, which provides a data structure that is compatible with the machine learning aglorithms in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/day3/bag_of_words.svg\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that each sample in your dataset is represented as one string, which could be just a sentence, an email, or a whole news article or book. To represent the sample, we first split the string into a list of tokens, which correspond to (somewhat normalized) words. A simple way to do this to just split by whitespace, and then lowercase the word. \n",
    "\n",
    "Then, we build a vocabulary of all tokens (lowercased words) that appear in our whole dataset. This is usually a very large vocabulary.\n",
    "Finally, looking at our single sample, we could show how often each word in the vocabulary appears.\n",
    "We represent our string by a vector, where each entry is how often a given word in the vocabulary appears in the string.\n",
    "\n",
    "As each sample will only contain very few words, most entries will be zero, leading to a very high-dimensional but sparse representation.\n",
    "\n",
    "The method is called \"bag-of-words,\" as the order of the words is lost entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:59:44.098975Z",
     "start_time": "2019-05-31T13:59:44.079299Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[~df[\"Summary\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:59:44.603336Z",
     "start_time": "2019-05-31T13:59:44.589980Z"
    }
   },
   "outputs": [],
   "source": [
    "X_raw = df[\"Summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:59:45.940450Z",
     "start_time": "2019-05-31T13:59:45.539762Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=500) # top 500 most frequent words\n",
    "vectorizer.fit(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:59:46.882465Z",
     "start_time": "2019-05-31T13:59:46.442943Z"
    }
   },
   "outputs": [],
   "source": [
    "X_bag_of_words = vectorizer.transform(X_raw).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:59:47.397057Z",
     "start_time": "2019-05-31T13:59:47.338116Z"
    }
   },
   "outputs": [],
   "source": [
    "wordCounts = zip(vectorizer.get_feature_names(), np.sum(X_bag_of_words, axis=0))\n",
    "pd.DataFrame([{\"word\":word, \"count\":count} for word,count in wordCounts]).sort_values(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:59:47.948531Z",
     "start_time": "2019-05-31T13:59:47.937733Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.inverse_transform(X_bag_of_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful transformation that is often applied to the bag-of-word encoding is the so-called term-frequency inverse-document-frequency (tf-idf) scaling, which is a non-linear transformation of the word counts.\n",
    "\n",
    "The tf-idf encoding rescales words that are common to have less weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:59:53.375250Z",
     "start_time": "2019-05-31T13:59:53.031863Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "tfidf_vectorizer.fit(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:59:54.290862Z",
     "start_time": "2019-05-31T13:59:53.888774Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tfidf = tfidf_vectorizer.transform(X_raw).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:59:54.889236Z",
     "start_time": "2019-05-31T13:59:54.757844Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X_bag_of_words\n",
    "y = df[\"Score\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T14:02:19.944175Z",
     "start_time": "2019-05-31T14:02:19.486922Z"
    }
   },
   "outputs": [],
   "source": [
    "regressor = Ridge()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T14:02:20.722821Z",
     "start_time": "2019-05-31T14:02:20.683660Z"
    }
   },
   "outputs": [],
   "source": [
    "wordCoef = pd.DataFrame({\"word\":list(vectorizer.get_feature_names()), \"coef\":regressor.coef_})\n",
    "#wordCoef[\"abs\"] = wordCoef[\"coef\"].abs()\n",
    "wordCoef = wordCoef.sort_values(\"coef\", ascending=False)\n",
    "wordCoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T14:02:28.100489Z",
     "start_time": "2019-05-31T14:02:27.998875Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_train = regressor.predict(X_train)\n",
    "regressor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "\n",
    "Try to use the TF-IDF encoder to see if you obtain better results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T14:02:35.869721Z",
     "start_time": "2019-05-31T14:02:35.456267Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/day3-04-01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "\n",
    "Predict the score for the test_sentences defined below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"Great coffee. yummy\",\n",
    "    \"This pork is disgusting\",\n",
    "    \"dangerous and nasty\",\n",
    "    \"look expired\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:44:33.777053Z",
     "start_time": "2019-05-31T13:44:33.768691Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/day3-04-02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "\n",
    "Try to use the review text instead of the review summary to see if we can improve the score.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "\n",
    "Try to combine the review text with the review summary to see if we can improve the score further.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
